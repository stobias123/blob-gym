{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from blob_env.blob_env import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='Blob2d-v1',\n",
    "    entry_point='blob_env.blob_env:BlobEnv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlobEnv()\n",
    "example_env = BlobEnv()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load('Blob2d-v1'))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(suite_gym.load('Blob2d-v1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 200000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = .1  # @param {type:\"number\"}\n",
    "log_interval = 1000  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                                tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_returns = []\n",
    "def compute_avg_return(environment, policy, num_episodes=100):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "    episode_returns = []\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env.reset()\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(tf_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1000: loss = 22.408761978149414\n",
      "step = 1000: Average Return = -1.0\n",
      "step = 2000: loss = 12.162713050842285\n",
      "step = 2000: Average Return = 1.600000023841858\n",
      "step = 3000: loss = 9.425508499145508\n",
      "step = 3000: Average Return = -1.0\n",
      "step = 4000: loss = 12.46407699584961\n",
      "step = 4000: Average Return = -1.0\n",
      "step = 5000: loss = 5.286794185638428\n",
      "step = 5000: Average Return = -1.0\n",
      "step = 6000: loss = 18.408531188964844\n",
      "step = 6000: Average Return = -30.899999618530273\n",
      "step = 7000: loss = 1.582027792930603\n",
      "step = 7000: Average Return = -1.0\n",
      "step = 8000: loss = 3.266300678253174\n",
      "step = 8000: Average Return = -1.0\n",
      "step = 9000: loss = 1.809601068496704\n",
      "step = 9000: Average Return = -1.0\n",
      "step = 10000: loss = 5.318526268005371\n",
      "step = 10000: Average Return = -1.0\n",
      "step = 11000: loss = 14.190179824829102\n",
      "step = 11000: Average Return = 1.600000023841858\n",
      "step = 12000: loss = 3.689124822616577\n",
      "step = 12000: Average Return = -30.899999618530273\n",
      "step = 13000: loss = 23.070192337036133\n",
      "step = 13000: Average Return = -1.0\n",
      "step = 14000: loss = 13.868996620178223\n",
      "step = 14000: Average Return = -1.0\n",
      "step = 15000: loss = 1.0169795751571655\n",
      "step = 15000: Average Return = -1.0\n",
      "step = 16000: loss = 2.086533784866333\n",
      "step = 16000: Average Return = -1.0\n",
      "step = 17000: loss = 3.70257830619812\n",
      "step = 17000: Average Return = -1.0\n",
      "step = 18000: loss = 1384.84912109375\n",
      "step = 18000: Average Return = -1.0\n",
      "step = 19000: loss = 7.679718017578125\n",
      "step = 19000: Average Return = -1.0\n",
      "step = 20000: loss = 9.576499938964844\n",
      "step = 20000: Average Return = -1.0\n",
      "step = 21000: loss = 20.61546516418457\n",
      "step = 21000: Average Return = -1.0\n",
      "step = 22000: loss = 54.202171325683594\n",
      "step = 22000: Average Return = -1.0\n",
      "step = 23000: loss = 2.3406832218170166\n",
      "step = 23000: Average Return = -1.0\n",
      "step = 24000: loss = 46.86248016357422\n",
      "step = 24000: Average Return = -1.0\n",
      "step = 25000: loss = 6.578358173370361\n",
      "step = 25000: Average Return = -1.0\n",
      "step = 26000: loss = 7.023965835571289\n",
      "step = 26000: Average Return = -1.0\n",
      "step = 27000: loss = 14.825372695922852\n",
      "step = 27000: Average Return = -28.299999237060547\n",
      "step = 28000: loss = 1.8393363952636719\n",
      "step = 28000: Average Return = -1.0\n",
      "step = 29000: loss = 11.054435729980469\n",
      "step = 29000: Average Return = -1.0\n",
      "step = 30000: loss = 5.329426288604736\n",
      "step = 30000: Average Return = -1.0\n",
      "step = 31000: loss = 14.65626049041748\n",
      "step = 31000: Average Return = -1.0\n",
      "step = 32000: loss = 1388.81103515625\n",
      "step = 32000: Average Return = -1.0\n",
      "step = 33000: loss = 8.569903373718262\n",
      "step = 33000: Average Return = -30.899999618530273\n",
      "step = 34000: loss = 2766.803955078125\n",
      "step = 34000: Average Return = -1.0\n",
      "step = 35000: loss = 15.46460247039795\n",
      "step = 35000: Average Return = -1.0\n",
      "step = 36000: loss = 4.009915828704834\n",
      "step = 36000: Average Return = -1.0\n",
      "step = 37000: loss = 6.23342227935791\n",
      "step = 37000: Average Return = -1.0\n",
      "step = 38000: loss = 14.242164611816406\n",
      "step = 38000: Average Return = -1.0\n",
      "step = 39000: loss = 1.666879415512085\n",
      "step = 39000: Average Return = -1.0\n",
      "step = 40000: loss = 3.5139524936676025\n",
      "step = 40000: Average Return = -1.0\n",
      "step = 41000: loss = 1404.942138671875\n",
      "step = 41000: Average Return = -1.0\n",
      "step = 42000: loss = 6.742999076843262\n",
      "step = 42000: Average Return = -1.0\n",
      "step = 43000: loss = 2.2067155838012695\n",
      "step = 43000: Average Return = -1.0\n",
      "step = 44000: loss = 7.919027805328369\n",
      "step = 44000: Average Return = -1.0\n",
      "step = 45000: loss = 53.08319854736328\n",
      "step = 45000: Average Return = -1.0\n",
      "step = 46000: loss = 3.1349000930786133\n",
      "step = 46000: Average Return = 1.600000023841858\n",
      "step = 47000: loss = 1385.327392578125\n",
      "step = 47000: Average Return = -1.0\n",
      "step = 48000: loss = 1.2713987827301025\n",
      "step = 48000: Average Return = -1.0\n",
      "step = 49000: loss = 1403.3218994140625\n",
      "step = 49000: Average Return = 4.199999809265137\n",
      "step = 50000: loss = 14.930109977722168\n",
      "step = 50000: Average Return = -1.0\n",
      "step = 51000: loss = 1.9189711809158325\n",
      "step = 51000: Average Return = -1.0\n",
      "step = 52000: loss = 0.9137663841247559\n",
      "step = 52000: Average Return = -1.0\n",
      "step = 53000: loss = 51.940162658691406\n",
      "step = 53000: Average Return = -30.899999618530273\n",
      "step = 54000: loss = 299.9840393066406\n",
      "step = 54000: Average Return = -1.0\n",
      "step = 55000: loss = 1.2265784740447998\n",
      "step = 55000: Average Return = -1.0\n",
      "step = 56000: loss = 11.890368461608887\n",
      "step = 56000: Average Return = -1.0\n",
      "step = 57000: loss = 812.9725341796875\n",
      "step = 57000: Average Return = -1.0\n",
      "step = 58000: loss = 5.992478370666504\n",
      "step = 58000: Average Return = -1.0\n",
      "step = 59000: loss = 3.7297215461730957\n",
      "step = 59000: Average Return = -1.0\n",
      "step = 60000: loss = 5.516154766082764\n",
      "step = 60000: Average Return = -1.0\n",
      "step = 61000: loss = 7.661470413208008\n",
      "step = 61000: Average Return = -1.0\n",
      "step = 62000: loss = 16.421096801757812\n",
      "step = 62000: Average Return = -1.0\n",
      "step = 63000: loss = 3.271329879760742\n",
      "step = 63000: Average Return = 1.600000023841858\n",
      "step = 64000: loss = 24.717304229736328\n",
      "step = 64000: Average Return = -30.899999618530273\n",
      "step = 65000: loss = 6.688525199890137\n",
      "step = 65000: Average Return = -1.0\n",
      "step = 66000: loss = 2.129246234893799\n",
      "step = 66000: Average Return = -1.0\n",
      "step = 67000: loss = 1.4531669616699219\n",
      "step = 67000: Average Return = -1.0\n",
      "step = 68000: loss = 1.2867847681045532\n",
      "step = 68000: Average Return = -1.0\n",
      "step = 69000: loss = 2.655564308166504\n",
      "step = 69000: Average Return = -1.0\n",
      "step = 70000: loss = 34.624610900878906\n",
      "step = 70000: Average Return = -1.0\n",
      "step = 71000: loss = 1.4507477283477783\n",
      "step = 71000: Average Return = -1.0\n",
      "step = 72000: loss = 25.300464630126953\n",
      "step = 72000: Average Return = -1.0\n",
      "step = 73000: loss = 1.3090484142303467\n",
      "step = 73000: Average Return = -1.0\n",
      "step = 74000: loss = 1396.92138671875\n",
      "step = 74000: Average Return = -30.899999618530273\n",
      "step = 75000: loss = 2.7916951179504395\n",
      "step = 75000: Average Return = -1.0\n",
      "step = 76000: loss = 2.193789005279541\n",
      "step = 76000: Average Return = -1.0\n",
      "step = 77000: loss = 3.2151670455932617\n",
      "step = 77000: Average Return = -1.0\n",
      "step = 78000: loss = 779.4638061523438\n",
      "step = 78000: Average Return = -1.0\n",
      "step = 79000: loss = 17.469728469848633\n",
      "step = 79000: Average Return = -1.0\n",
      "step = 80000: loss = 1.859328269958496\n",
      "step = 80000: Average Return = -1.0\n",
      "step = 81000: loss = 8.95606803894043\n",
      "step = 81000: Average Return = -1.0\n",
      "step = 82000: loss = 1400.313720703125\n",
      "step = 82000: Average Return = -1.0\n",
      "step = 83000: loss = 6.886819839477539\n",
      "step = 83000: Average Return = -30.899999618530273\n",
      "step = 84000: loss = 2.212401866912842\n",
      "step = 84000: Average Return = -1.0\n",
      "step = 85000: loss = 1383.121337890625\n",
      "step = 85000: Average Return = -1.0\n",
      "step = 86000: loss = 25.2194881439209\n",
      "step = 86000: Average Return = -1.0\n",
      "step = 87000: loss = 1.3701292276382446\n",
      "step = 87000: Average Return = -1.0\n",
      "step = 88000: loss = 16.83031463623047\n",
      "step = 88000: Average Return = -1.0\n",
      "step = 89000: loss = 1.8539623022079468\n",
      "step = 89000: Average Return = -1.0\n",
      "step = 90000: loss = 2765.4765625\n",
      "step = 90000: Average Return = 1.600000023841858\n",
      "step = 91000: loss = 137.75457763671875\n",
      "step = 91000: Average Return = 1.600000023841858\n",
      "step = 92000: loss = 1.4316831827163696\n",
      "step = 92000: Average Return = -30.899999618530273\n",
      "step = 93000: loss = 14.772517204284668\n",
      "step = 93000: Average Return = -1.0\n",
      "step = 94000: loss = 2.8844263553619385\n",
      "step = 94000: Average Return = -1.0\n",
      "step = 95000: loss = 1.7316075563430786\n",
      "step = 95000: Average Return = -1.0\n",
      "step = 96000: loss = 1383.9049072265625\n",
      "step = 96000: Average Return = -1.0\n",
      "step = 97000: loss = 2.361186981201172\n",
      "step = 97000: Average Return = -1.0\n",
      "step = 98000: loss = 5.152968406677246\n",
      "step = 98000: Average Return = -1.0\n",
      "step = 99000: loss = 3.491847276687622\n",
      "step = 99000: Average Return = -1.0\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 100000 # @param {type:\"integer\"}\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "losses = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(tf_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "    losses.append(train_loss)\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.plot(iterations, losses)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=1,bottom=-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_py_env = suite_gym.load('Blob2d-v1')\n",
    "video_env = tf_py_environment.TFPyEnvironment(video_py_env)\n",
    "\n",
    "def create_policy_eval_video(policy, filename, num_episodes=50, fps=5):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = video_env.reset()\n",
    "      video.append_data(video_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = video_env.step(action_step.action)\n",
    "        video.append_data(video_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
