{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from blob_env.blob_env import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='Blob2d-v1',\n",
    "    entry_point='blob_env.blob_env:BlobEnv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlobEnv()\n",
    "example_env = BlobEnv()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load('Blob2d-v1'))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(suite_gym.load('Blob2d-v1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                                tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_returns = p[]\n",
    "def compute_avg_return(environment, policy, num_episodes=100):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "    episode_returns = []\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  print(\"[AVG Return] \" + avg_return)\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-36.8"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "eval_env.reset()\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(10, 10, 3), dtype=tf.uint8, name='observation', minimum=array(0, dtype=uint8), maximum=array(255, dtype=uint8)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(8)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(tf_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/vscode/.local/lib/python3.6/site-packages/tensorflow/python/autograph/operators/control_flow.py:1004: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Trajectory(step_type=<tf.Tensor: shape=(), dtype=int32, numpy=0>, observation=<tf.Tensor: shape=(10, 10, 3), dtype=uint8, numpy=\n",
       " array([[[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [255, 175,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0, 255],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0, 255,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]]], dtype=uint8)>, action=<tf.Tensor: shape=(), dtype=int64, numpy=8>, policy_info=(), next_step_type=<tf.Tensor: shape=(), dtype=int32, numpy=2>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.0>, discount=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>),\n",
       " BufferInfo(ids=<tf.Tensor: shape=(), dtype=int64, numpy=44>, probabilities=<tf.Tensor: shape=(), dtype=float32, numpy=0.01>))"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 10, 10, 3), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.uint8, action=tf.int64, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fcef4694e10>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/vscode/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 200: loss = 1353.4427490234375\n",
      "step = 400: loss = 0.678589940071106\n",
      "step = 600: loss = 0.5444076061248779\n",
      "step = 800: loss = 0.2066888064146042\n",
      "step = 1000: loss = 1.1121361255645752\n",
      "step = 1000: Average Return = -1.0\n",
      "step = 1200: loss = 13.000885963439941\n",
      "step = 1400: loss = 12.54322624206543\n",
      "step = 1600: loss = 5.34168004989624\n",
      "step = 1800: loss = 1.430657148361206\n",
      "step = 2000: loss = 2.820723056793213\n",
      "step = 2000: Average Return = -1.0\n",
      "step = 2200: loss = 3.0593061447143555\n",
      "step = 2400: loss = 1.528198003768921\n",
      "step = 2600: loss = 4.5570878982543945\n",
      "step = 2800: loss = 0.9297044277191162\n",
      "step = 3000: loss = 28.440017700195312\n",
      "step = 3000: Average Return = 1.600000023841858\n",
      "step = 3200: loss = 4.118219375610352\n",
      "step = 3400: loss = 10.327183723449707\n",
      "step = 3600: loss = 12.928516387939453\n",
      "step = 3800: loss = 3.047591209411621\n",
      "step = 4000: loss = 7.588883399963379\n",
      "step = 4000: Average Return = -1.0\n",
      "step = 4200: loss = 2.586585760116577\n",
      "step = 4400: loss = 3.08833384513855\n",
      "step = 4600: loss = 7.411620616912842\n",
      "step = 4800: loss = 9.314421653747559\n",
      "step = 5000: loss = 51.95682144165039\n",
      "step = 5000: Average Return = 1.600000023841858\n",
      "step = 5200: loss = 5.3442487716674805\n",
      "step = 5400: loss = 7.015336990356445\n",
      "step = 5600: loss = 1.664827585220337\n",
      "step = 5800: loss = 3.4031314849853516\n",
      "step = 6000: loss = 2.7088048458099365\n",
      "step = 6000: Average Return = -1.0\n",
      "step = 6200: loss = 3.166411876678467\n",
      "step = 6400: loss = 18.794353485107422\n",
      "step = 6600: loss = 1.5780917406082153\n",
      "step = 6800: loss = 1.1994593143463135\n",
      "step = 7000: loss = 4.992472171783447\n",
      "step = 7000: Average Return = -1.0\n",
      "step = 7200: loss = 14.48414421081543\n",
      "step = 7400: loss = 2.545710563659668\n",
      "step = 7600: loss = 1067.4498291015625\n",
      "step = 7800: loss = 5.0929975509643555\n",
      "step = 8000: loss = 1.7284005880355835\n",
      "step = 8000: Average Return = 1.600000023841858\n",
      "step = 8200: loss = 2.0727834701538086\n",
      "step = 8400: loss = 19.70979881286621\n",
      "step = 8600: loss = 1.8975807428359985\n",
      "step = 8800: loss = 1.9834829568862915\n",
      "step = 9000: loss = 8.595057487487793\n",
      "step = 9000: Average Return = -1.0\n",
      "step = 9200: loss = 4.751159191131592\n",
      "step = 9400: loss = 1.0124108791351318\n",
      "step = 9600: loss = 2.3398783206939697\n",
      "step = 9800: loss = 4.070598125457764\n",
      "step = 10000: loss = 3.1879773139953613\n",
      "step = 10000: Average Return = -1.0\n",
      "step = 10200: loss = 6.65192985534668\n",
      "step = 10400: loss = 4.815334320068359\n",
      "step = 10600: loss = 28.88727569580078\n",
      "step = 10800: loss = 19.623212814331055\n",
      "step = 11000: loss = 9.560627937316895\n",
      "step = 11000: Average Return = -30.899999618530273\n",
      "step = 11200: loss = 14.257049560546875\n",
      "step = 11400: loss = 1.7662936449050903\n",
      "step = 11600: loss = 2.3695926666259766\n",
      "step = 11800: loss = 15.317740440368652\n",
      "step = 12000: loss = 1.0354009866714478\n",
      "step = 12000: Average Return = -1.0\n",
      "step = 12200: loss = 2.569798469543457\n",
      "step = 12400: loss = 1.4124146699905396\n",
      "step = 12600: loss = 8.929473876953125\n",
      "step = 12800: loss = 3.6321909427642822\n",
      "step = 13000: loss = 5.384121417999268\n",
      "step = 13000: Average Return = -1.0\n",
      "step = 13200: loss = 3.4424619674682617\n",
      "step = 13400: loss = 4.1280717849731445\n",
      "step = 13600: loss = 2.224820375442505\n",
      "step = 13800: loss = 1.1577790975570679\n",
      "step = 14000: loss = 3.220791816711426\n",
      "step = 14000: Average Return = 1.600000023841858\n",
      "step = 14200: loss = 1.4769994020462036\n",
      "step = 14400: loss = 0.6043061017990112\n",
      "step = 14600: loss = 5.6090545654296875\n",
      "step = 14800: loss = 4.069892883300781\n",
      "step = 15000: loss = 5.622364521026611\n",
      "step = 15000: Average Return = -30.899999618530273\n",
      "step = 15200: loss = 11.013851165771484\n",
      "step = 15400: loss = 1.5060572624206543\n",
      "step = 15600: loss = 16.095706939697266\n",
      "step = 15800: loss = 0.8023928999900818\n",
      "step = 16000: loss = 1.4092986583709717\n",
      "step = 16000: Average Return = -1.0\n",
      "step = 16200: loss = 2.6091983318328857\n",
      "step = 16400: loss = 1.89898681640625\n",
      "step = 16600: loss = 2.8016791343688965\n",
      "step = 16800: loss = 2.2681779861450195\n",
      "step = 17000: loss = 4.63676118850708\n",
      "step = 17000: Average Return = -1.0\n",
      "step = 17200: loss = 3.7883596420288086\n",
      "step = 17400: loss = 2.3643176555633545\n",
      "step = 17600: loss = 2.8638803958892822\n",
      "step = 17800: loss = 4.294459819793701\n",
      "step = 18000: loss = 5.804363250732422\n",
      "step = 18000: Average Return = 1.600000023841858\n",
      "step = 18200: loss = 2.8080320358276367\n",
      "step = 18400: loss = 6.720853805541992\n",
      "step = 18600: loss = 1.2826262712478638\n",
      "step = 18800: loss = 5.837840557098389\n",
      "step = 19000: loss = 4.075833320617676\n",
      "step = 19000: Average Return = 1.600000023841858\n",
      "step = 19200: loss = 1.786409854888916\n",
      "step = 19400: loss = 0.6286513805389404\n",
      "step = 19600: loss = 3.430020809173584\n",
      "step = 19800: loss = 7.019287586212158\n",
      "step = 20000: loss = 1396.1751708984375\n",
      "step = 20000: Average Return = -1.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(tf_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}